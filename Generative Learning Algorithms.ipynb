{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成学习算法\n",
    "\n",
    "以线性回归、逻辑回归为代表的广义线性模型，都是在针对 $p(y|x;\\theta)$，也就是给定 $x$ 时 $y$ 的条件概率分布进行建模。\n",
    "\n",
    "以分类问题为例，给定一个训练集，逻辑回归，试图寻找一条直线——也就是决策边界——来区分正类和反类。模型建立后，对于一个新的测试样本，给定其特征，我们判定这个样本落在决策边界的哪一侧，然后据此做出预测。\n",
    "\n",
    "直接学习 $p(y|x)$ 的算法，或者说直接将输入空间 $\\chi$ 映射到标签 $\\{0, 1\\}$的算法，称为**判别学习算法 Discriminative Learning Algorithm**。与此相反，针对 $p(x|y)$ 建模的算法，称为**生成学习算法 Generative Learning Algorithm**。\n",
    "\n",
    "还是以分类问题为例，生成学习算法通过对训练集的学习，给定一个分类 $y \\in \\{0, 1\\}$，我们的模型试图描述这个分类对应的特征的分布。\n",
    "\n",
    "通过对 $p(y)$ （称为**先验概率 class prior**）和 $p(x|y)$ 建模，我们之后通过贝叶斯法则来计算给定 $x$ 时 $y$ 的后验分布：\n",
    "$$ p(y|x) = \\frac{p(x|y)p(y)}{p(x)} $$\n",
    "\n",
    "其中，分母 $p(x) = p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$。而实际上，在预测时，我们并不需要计算 $p(x)$，因为\n",
    "$$\n",
    "\\begin{split}\n",
    "arg \\max_yp(y|x) &= arg \\max_y\\frac{p(x|y)p(y)}{p(x)} \\\\\n",
    "&= arg \\max_yp(x|y)p(y)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "本节包括以下内容：\n",
    "1. 高斯判别分析 Gaussian discriminant analysis\n",
    "2. 朴素贝叶斯 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. 高斯判别分析 Gaussian Discriminant Analysis\n",
    "\n",
    "在高斯判别分析中，我们假设 $p(x|y)$ 服从多维正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 多维正态分布 The MultiVariate Normal Distribution\n",
    "\n",
    "多维正态分布也称作多维高斯分布，其参数包括**均值向量 mean vector** $\\mu \\in \\mathbb{R}^n$ 以及**协方差矩阵 covariance matrix** $\\Sigma \\in \\mathbb{R}^{n \\times n}$。\n",
    "\n",
    "关于多维正态分布的具体特征，可参见[随机变量及其数字特征-矩、协方差矩阵](http://nbviewer.jupyter.org/github/reata/ProbabilityAndStatistics/blob/master/Mathematical%20Properties%20of%20Random%20Variable.ipynb)，这里略过详细介绍。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 高斯判别分析模型 The Gaussian Discriminant Analysis Model\n",
    "\n",
    "对于特征 $x$ 为连续型变量的分类问题，我们可以使用高斯判别分析（GDA）模型。该模型的假设包括：\n",
    "$$\n",
    "\\begin{split}\n",
    "y &\\sim Bernoulli(\\phi) \\\\\n",
    "x|y=0 &\\sim N(\\mu_0, \\Sigma) \\\\\n",
    "x|y=1 &\\sim N(\\mu_1, \\Sigma) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "对应的概率质量函数/概率密度函数为\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y) &= \\phi^y(1-\\phi)^{1-y} \\\\\n",
    "p(x|y=0) &= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_0)^{T}\\Sigma^{-1}(x-\\mu_0)) \\\\\n",
    "p(x|y=1) &= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_1)^{T}\\Sigma^{-1}(x-\\mu_1)) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "这里模型的参数为 $\\phi, \\Sigma, \\mu_0, \\mu_1$。（注意到尽管我们使用了不同的均值向量 $\\mu_0$ 和 $\\mu_1$，这里通常只使用同一个协方差矩阵 $\\Sigma$）其对数似然函数为\n",
    "$$\n",
    "\\begin{split}\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) &= log\\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma) \\\\\n",
    "&= log\\prod_{i=1}^m p(x^{(i)}|y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)};\\phi)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "使 $\\ell$ 最大的各参数分别为\n",
    "$$\n",
    "\\begin{split}\n",
    "\\phi &= \\frac{1}{m}\\sum_{i=1}^m 1\\{y^{(i)}=1\\} \\\\\n",
    "\\mu_0 &= \\frac{\\sum_{i=1}^m 1\\{y^{(i)}=0\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)}=0\\}} \\\\\n",
    "\\mu_1 &= \\frac{\\sum_{i=1}^m 1\\{y^{(i)}=1\\}x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)}=1\\}} \\\\\n",
    "\\Sigma &= \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}-\\mu_{y^{(i)}})(x^{(i)}-\\mu_{y^{(i)}})^T\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "由于使用了相同的协方差，最终根据 $y=1$ 和 $y=0$ 估计出的多维高斯分布，形状将完全相同，区别仅仅在于均值。而两个高斯分布的等差线相交的地方，就是高斯判别分析模型的决策边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 高斯判别分析和逻辑回归 GDA and Logistic Regression\n",
    "\n",
    "高斯判别分析和逻辑回归之间，存在着非常有意思的关联。如果我们将 $p(y=1|x;\\phi, \\mu_0, \\mu_1, \\Sigma)$ 视为 $x$ 的函数，即\n",
    "$$ p(y=1|x) = \\frac{p(x|y=1)p(y=1)}{p(x|y=1) + p(x|y=0)} $$\n",
    "\n",
    "则这个函数可以写成\n",
    "$$ p(y=1|x;\\phi, \\mu_0, \\mu_1, \\Sigma) = \\frac{1}{1+exp(-\\theta^Tx)} $$\n",
    "的形式，其中，$\\theta$ 是 $\\phi, \\Sigma, \\mu_0, \\mu_1$ 的函数。这正是作为一种判别学习算法的逻辑回归对 $p(y=1|x)$ 的建模形式。\n",
    "\n",
    "但是，高斯判别分析和逻辑回归求出的决策边界通常是不同的，也就是说 $\\theta$ 的值是不同的，该如何区分两种模型的优劣呢？\n",
    "\n",
    "注意我们上面的结论是，当 $p(x|y)$ 服从多维高斯分布时，则 $p(y|x)$ 必然是sigmoid函数的形式。而这个命题的逆命题是错误的，也就是说，当 $p(y|x)$ 是sigmoid函数的形式时，并不能得出 $p(x|y)$ 服从多维高斯分布。换言之，高斯判别分析的假设，比逻辑回归更强。\n",
    "\n",
    "当高斯判别分析的假设成立时，它会找到比逻辑回归更好的模型，高斯判别分析是一个**渐进有效估计量 asymptotically efficient estimator**，渐进有效估计量的含义是，在大样本的条件下，没有其它算法可以严格意义上比高斯判别分析更准确地预测 $p(y|x)$。但同时，当假设成立，即使数据量小，高斯判别分析的效果通常也好于逻辑回归。\n",
    "\n",
    "相反，由于使用了更弱的假设约束，逻辑回归会更健壮，不容易受到错误假设的影响。有很多种不同的假设（其中包括所有的指数分布族），都会使得 $p(y|x)$ 呈现sigmoid函数的形态。譬如一个数据集的 $p(x|y)$ 服从泊松分布，其 $p(y|x)$ 为sigmoid函数，当我们使用高斯判别分析来建模时，最终结果会很难预料，但我们可以预计逻辑回归仍然可以正常预测。\n",
    "\n",
    "总结来说，当数据并不是高斯分布时，数据量较大的情况下，逻辑回归几乎总是比高斯判别分析表现得更好。出于这个原因，在实际项目中逻辑回归的使用远远多于高斯判别分析。这个区别，或多或少也是判别算法和生成算法的区别，机关如此，我们接下来介绍的朴素贝叶斯算法，依然是非常经典且流行的分类算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. 朴素贝叶斯 Naive Bayes\n",
    "\n",
    "在高斯判别分析中，特征向量 $x$ 是连续型的实数向量。而朴素贝叶斯算法则用于处理特征向量 $x$ 是离散型的情况，朴素贝叶斯的最常见应用场景是以垃圾邮件判定为代表的**文本分类 text classification**问题。\n",
    "\n",
    "我们要用一个特征向量来表示给定的文本（比如一封email、一则新闻），首先我们需要构造一个**词典 vocabulary**。将真正意义上的字典里所有的词都列进来当然最为理想，但这会使特征矩阵非常大。习惯上，我们会从整个训练集中抽取所有出现的词（或者进一步，根据TF-IDF词频等算法，抽取权重较高的词）排序后作为词典，中文在抽取过程中还需要首先分词。对于一个训练样本来说，如果其包含词典中第 $i$ 个词，则 $x_i=1$，否则 $x_i=0$。\n",
    "\n",
    "比如一个词典，包含三个词a, buy, zoo，则向量 $[1, 0, 1]$ 表示一个样本，包含关键词a, zoo，不包含关键词buy。\n",
    "\n",
    "由特征向量，我们需要构造一个生成模型，对 $p(x|y)$ 建模。设想，我们的词典中包含50000个词，那么 $x \\in \\{0, 1\\}^{50000}$ （$x$ 是一个 $50000$ 维的向量，所有的元素只取 $0$ 或 $1$），如果要用多项分布的假设表示 $2^50000$ 种可能的结果，最终我们需要一个 $2^{50000}-1$ 维的参数向量，这是无法接受的。\n",
    "\n",
    "所以我们需要一个更强的假设，我们假设 $x_i$ 在给定 $y$ 时相互独立。这个假设称作**贝叶斯假设 Naive Bayes(NB) assumption**，基于这个假设的算法叫做**贝叶斯分类器 Naive Bayes classifier**。举个例子，$y=1$ 表示垃圾邮件，buy是词典中第2087个单词，price是词典中第39831个单词，那么我们的假设是，给定一则垃圾邮件（$y=1$），$x_{2087}$ 的取值（buy是否出现在该邮件中）与 $x_{39831}$ 的取值（price是否出现在该邮件中）不相关，即 $p(x_{2087}|y)=p(x_{2087}|y,x_{39831})$。注意这里的假设并不是 $x_{2087}$ 和 ${x_{39831}}$ 相互独立（$p(x_{2087})=p(x_{2087}|x_{39831})$），而是在给定 $y$ 的条件下相互独立。我们有\n",
    "$$\n",
    "\\begin{split}\n",
    "p(x_1, ..., x_{50000}|y) &= p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2) \\cdot\\cdot\\cdot p(x_{50000}|y,x_1,...,x_{49999}) \\\\\n",
    "&=p(x_1|y)p(x_2|y)p(x_3|y) \\cdot\\cdot\\cdot p(x_{50000}|y) \\\\\n",
    "&=\\prod_{i=1}^n p(x_i|y)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "在实际使用中，尽管贝叶斯假设十分严格，但贝叶斯分类器的表现通常较好。\n",
    "\n",
    "这里，模型的参数包括 $\\phi_{i|y=1}=p(x_i=1|y=1), \\phi_{i|y=0}=p(x_i=1|y=0), \\phi_y=p(y=1)$，给定训练集 $\\{(x^{(i)}, y^{(i)}); i=1,...,m\\}$，根据样本可以写成联合最大似然函数如下：\n",
    "\n",
    "$$L(\\phi_y,\\phi_{i|y=0},\\phi_{i|y=1}) = \\prod_{i=1}^m p(x^{(i)}, y^{(i)})$$\n",
    "\n",
    "取得最大值时各参数的取值如下：\n",
    "$$\n",
    "\\begin{split}\n",
    "\\phi_{j|y=1} &= \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)}=1 \\land y_j^{(i)}=1 \\}}{\\sum_{i=1}^m 1\\{y_j^{(i)}=1\\}} \\\\\n",
    "\\phi_{j|y=0} &= \\frac{\\sum_{i=1}^m 1\\{x_j^{(i)}=1 \\land y_j^{(i)}=0 \\}}{\\sum_{i=1}^m 1\\{y_j^{(i)}=0\\}} \\\\\n",
    "\\phi_y &= \\frac{\\sum_{i=1}^m 1\\{y^{(i)}=1\\}}{m}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "有了以上的参数，给定特征 $x$，我们就可以计算其属于某个分类的概率，然后只要挑选概率最大的分类即可：\n",
    "$$\n",
    "\\begin{split}\n",
    "p(y=1|x) &= \\frac{p(x|y=1)p(y=1)}{p(x)} \\\\\n",
    "&= \\frac{(\\prod_{i=1}^n p(x_i|y=1))p(y=1)}{(\\prod_{i=1}^n p(x_i|y=1))p(y=1) + (\\prod_{i=1}^n p(x_i|y=0))p(y=0)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "注意到我们这里的朴素贝叶斯主要用于特征 $x$ 均为二元离散变量的情况，但模型可以非常容易地扩展到 $k$ 元离散变量的情况，我们只要假定 $p(x_i|y)$ 服从多项分布即可。即使对于连续型的特征，我们也可以通过分组的办法将其**离散化 discretize**，然后按照多项分布的方式来建模。尤其，当原先的连续型特征不能很好地满足多维正态分布的情况，特征离散化后使用朴素贝叶斯分类器的效果，通常会比高斯判别分析更好。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
