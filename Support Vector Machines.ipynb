{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机 Support Vector Machines\n",
    "\n",
    "本节包括以下内容：\n",
    "1. 间隔 Margin\n",
    "2. 记号 Notation\n",
    "3. 函数间隔和几何间隔 Functional and geometric margins\n",
    "4. 最大间隔分类器 The optimal margin classifier\n",
    "5. 拉格朗日对偶性 Lagrange duality\n",
    "6. 最大间隔分类器 Optimal margin classifier\n",
    "7. 核方法 Kernels\n",
    "8. 正则化、不可分的情况 Regularization and the non-separable case\n",
    "9. SMO算法 The SMO algorithm\n",
    "10. SMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. 间隔 Margin\n",
    "\n",
    "回忆线性回归，概率 $p(y=1|x; \\theta)$ 由 $h_\\theta(x) = g(\\theta^Tx)$ 计算而得。如果 $h_\\theta(x) \\geq 0.5$，则我们预测输入 $x$ 为正类，换言之，当且仅当 $\\theta^Tx \\geq 0$ 时，模型预测 $x$ 为正类。并且，根据我们的概率假设，$\\theta^Tx$ 越大，$x$ 取正类的概率就越大，或者说，我们对于预测 $x$ 为正类的信心就越大。\n",
    "\n",
    "也就是说，如果 $\\theta^Tx \\gg 0$，那么我们可以非常自信地预测 $y=1$。类似的，如果 $\\theta^Tx \\ll 0$，则我们可以非常自信地预测 $y=0$。直觉上看，如果我们的成本函数考虑这一点，那么我们可以得到一个不错的分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
