{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习和控制 Reinforcement Learning and Control\n",
    "\n",
    "在监督学习中，算法根据训练集，试图模仿类似标签 $y$ 的输出。在这个背景下，每一个输入 $x$，其标签都给出了一个毫无争议的正确答案。相反，对于许多连续决策和控制问题，很难给学习算法提供类似明确的监督。例如，假设我们造好令一个四足机器人，想要编程让其走路，那么起初我们对于走路所需要的行动毫无概念，也就无法为算法提供准确的监督。\n",
    "\n",
    "在强化学习框架中，我们只给学习算法提供**强化函数 reward function**，提示学习主体是学习得好或不好。对于四足机器人的例子，前进时强化函数给予机器人正向激励，后退或跌倒时给予负向激励。学习算法则需要在跨时间周期中学习如何行动以获得最大激励。\n",
    "\n",
    "强化学习在直升机自动驾驶、机器人行走、手机网络路由、市场战略选择、工厂管理、高效网页索引方面都有非常成功的应用。在介绍强化学习之前，我们首先将学习马尔可夫决策过程，马尔可夫决策过程为强化学习问题建立了正式的讨论框架。\n",
    "\n",
    "本节包括以下内容：\n",
    "1. 马尔可夫决策过程 Markov decision processes\n",
    "2. 值迭代和策略迭代 Value iteration and policy iteration\n",
    "3. 学习马尔可夫决策过程的模型 Learning a model for an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. 马尔可夫决策过程 Markov Decision Processes\n",
    "\n",
    "马尔可夫决策过程包含一组元素 $(S, A, \\{P_{sa}\\}, \\gamma, R)$，其中\n",
    "\n",
    "- $S$ 表示一组**状态 states**（在自动飞行器中，$S$ 表示表示飞行器所有可能的位置和方向）\n",
    "- $A$ 表示一组**动作 actions**（在自动飞行器中，表示飞行器控制手柄所有可能的方向）\n",
    "- $P_{sa}$ 表示状态迁移的概率。对每个状态 $s \\in S$ 以及每个动作 $a \\in A$，$P_{sa}$ 表示在状态空间内的分布。简单来说，$P_{sa}$ 给出了我们对状态 $s$ 施加动作 $a$ 之后将会迁移到的所有可能状态的概率分布。\n",
    "- $\\gamma \\in [0,1)$ 称为**贴现因子 discount factor**\n",
    "- $R: S \\times A \\rightarrow R$ 表示强化函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
